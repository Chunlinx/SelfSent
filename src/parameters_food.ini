[mode]
# Always let train to true. Will be remove in a future version
train_model = True
use_pretrained_model = False
pretrained_model_folder = ../trained_models/food
attention_visualization_conf = 0.8

[lib]
stanford_folder = ../lib/stanford-corenlp-full-2016-10-31

[dataset]
# if seed = -1, let the seed initialized at random
seed = 28111993

do_split = False
train_size = 754304
valid_size = 16384
test_size = 16384

dataset_folder = ../data/food

#---------------------------------------------------------------------------------------------------------------------#
# The parameters below are for advanced users. Their default values should yield good performance in most cases.      #
#---------------------------------------------------------------------------------------------------------------------#

[ann]
da = 350
r = 30
batch_size = 128

# Set to -1 to disable the sentence limitation
max_length_sentence = 500

# In order to use random initialization instead, set token_pretrained_embedding_filepath to empty string, as below:
# token_pretrained_embedding_filepath =
token_pretrained_embedding_filepath = ../data/word_vectors/glove.6B.100d.txt
token_embedding_dimension = 100
lstm_hidden_state_dimension = 300

mlp_hidden_layer_1_units = 3000
#mlp_hidden_layer_2_units = 3000

[training]
patience = 10
maximum_number_of_epochs = 100

# optimizer should be either 'sgd' or 'adam'
optimizer = adam
learning_rate = 0.001
beta_penalized = 1.0
beta_l2 = 0.0001

# gradients will be clipped above |gradient_clipping_value| and below -|gradient_clipping_value|, if gradient_clipping_value is non-zero
# (set to 0 to disable gradient clipping)
gradient_clipping_value = 0.5

# dropout_rate should be between 0 and 1
dropout_rate = 0.5

# Upper bound on the number of CPU threads SelfSent will use
number_of_cpu_threads = 8

# Upper bound on the number of GPU SelfSent will use
# If number_of_gpus > 0, you need to have installed tensorflow-gpu
number_of_gpus = 0
# if gpu = 1 and there are multiple gpus, let the user choose. -1 to disable it
gpu_device = 0

[advanced]
experiment_name = food

## If remap_unknown_tokens is set to True, map to UNK any token that hasn't been seen in neither the training set nor the pre-trained token embeddings.
remap_to_unk_count_threshold = 1
remap_unknown_tokens_to_unk = True

# If load_only_pretrained_token_embeddings is set to True, then token embeddings will only be loaded if it exists in token_pretrained_embedding_filepath
# or in pretrained_model_checkpoint_filepath, even for the training set.
load_only_pretrained_token_embeddings = False

# If check_for_lowercase is set to True, the lowercased version of each token will also be checked when loading the pretrained embeddings.
# For example, if the token 'Boston' does not exist in the pretrained embeddings, then it is mapped to the embedding of its lowercased version 'boston',
# if it exists among the pretrained embeddings.
check_for_lowercase = True

# If check_for_digits_replaced_with_zeros is set to True, each token with digits replaced with zeros will also be checked when loading pretrained embeddings.
# For example, if the token '123-456-7890' does not exist in the pretrained embeddings, then it is mapped to the embedding of '000-000-0000',
# if it exists among the pretrained embeddings.
# If both check_for_lowercase and check_for_digits_replaced_with_zeros are set to True, then the lowercased version is checked before the digit-zeroed version.
check_for_digits_replaced_with_zeros = True

# If freeze_token_embeddings is set to True, token embedding will remain frozen (not be trained).
freeze_token_embeddings = False

## If debug is set to True, only 200 lines will be loaded for each split of the dataset.
debug = False
verbose = False

# plot_format specifies the format of the plots generated by SelfSent. It should be either 'png' or 'pdf'.
plot_format = pdf
